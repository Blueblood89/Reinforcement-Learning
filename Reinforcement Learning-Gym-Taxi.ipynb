{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import gym","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* blue = passenger\n* purple = destination\n* yellow = empty taxi\n* green = full taxi\n* RGBY = location for destination and passanger"},{"metadata":{"_kg_hide-output":false,"trusted":true},"cell_type":"code","source":"env = gym.make(\"Taxi-v2\").env\nenv.render()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* reset env and return  random initial state"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"env.reset()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(env.observation_space)\nprint(env.action_space)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* taxi row, taxi column, passenger index, destination"},{"metadata":{"trusted":true},"cell_type":"code","source":"state = env.encode(3,1,2,3)\nprint(state)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"env.s = state\nenv.render()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 6 discrete deterministic actions:\n    - 0: move south\n    - 1: move north\n    - 2: move east \n    - 3: move west \n    - 4: pickup passenger\n    - 5: dropoff passenger\n    \nFirst column:probability, \nSecond column:next_state, \nThird column:reward, \nFourt column:done"},{"metadata":{"trusted":true},"cell_type":"code","source":"env.P[331]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Again, we must reset our environment."},{"metadata":{"trusted":true},"cell_type":"code","source":"env.reset()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total_reward_list = []\n# episode\nfor j in range(5):\n    env.reset()\n    time_step = 0\n    total_reward = 0\n    list_visualize = []\n    while True:\n        time_step += 1\n        #choose action\n        action = env.action_space.sample()\n        #perform action and get reward\n        state, reward, done, _ =  env.step(action) # state = next state\n        #total reward\n        total_reward += reward\n        # visualize\n        list_visualize.append({\"frame\": env.render(mode = \"ansi\"),\n                                \"state\": state, \"action\": action, \"reward\":reward,\n                                \"Total Reward\": total_reward})\n        if done:\n            total_reward_list.append(total_reward)\n            break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import time       \nfor i, frame in enumerate(list_visualize):\n    print(frame[\"frame\"])\n    print(\"Timestep: \", i + 1)\n    print(\"State: \", frame[\"state\"])\n    print(\"action: \", frame[\"action\"])\n    print(\"reward: \", frame[\"reward\"])\n    print(\"Total Reward: \", frame[\"Total Reward\"])\n    # time.sleep(2)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}
